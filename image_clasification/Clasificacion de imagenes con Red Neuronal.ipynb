{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Clasificacion de imagenes con Red Neuronal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiTaila/hack_iia/blob/ia-2003/image_clasification/Clasificacion%20de%20imagenes%20con%20Red%20Neuronal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD-cjEe4ydeG",
        "colab_type": "text"
      },
      "source": [
        "# Clasificación de imágenes con Red Neuronal\n",
        "\n",
        "La idea de este ejercicio es realizar la misma clasificación que hicimos con kNN sobre `CIFAR10`, pero utilizando una red neuronal de 2 capas implementada en `Python`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOUYgDU6ydeJ",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive, set directories and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5TRbcHaxjgU",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lUjd3mE5xjgZ",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/hack_iia/image_clasification/')  #change dir\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj8p1v6TydeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlORmpxoydeP",
        "colab_type": "text"
      },
      "source": [
        "## Red neuronal de dos capas\n",
        "\n",
        "Implementación de una red neuronal totalmente conectada (Fully Connected - FC) \n",
        "\n",
        "La capa de entrada tiene dimensión `N`, la capa oculta `H` y realiza una clasificación en `C` clases.\n",
        "\n",
        "Vamos a entrenar una red con `SoftMax Loss`, y `ReLu`.\n",
        "\n",
        "En otras palabras, la red tendrá la siguiente arquitectura:\n",
        "\n",
        "```\n",
        "entrada - capa totalmente conectada - ReLU - capa totalmente conectada - SoftMax\n",
        "```\n",
        "\n",
        "Las salidas de la segunda capa totalmente conectada representan los puntajes para cada clase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uSo-3-TydeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(object):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  def loss(self, X, y=None, reg=0.0):\n",
        "    W1, b1 = self.params['W1'], self.params['b1']\n",
        "    W2, b2 = self.params['W2'], self.params['b2']\n",
        "    N, D = X.shape\n",
        "\n",
        "    # Forward pass\n",
        "    scores = None\n",
        "\n",
        "    XW1 = np.dot(X, W1)\n",
        "    XW1pb1 = XW1 + b1\n",
        "    H1 = np.maximum(XW1pb1, 0)\n",
        "\n",
        "    H1W2 = np.dot(H1, W2)\n",
        "    scores = H1W2 + b2\n",
        "\n",
        "    if y is None:\n",
        "      return scores\n",
        "\n",
        "    # Loss\n",
        "    loss = None\n",
        "    scores -=  np.max(scores)\n",
        "    q = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
        "    loss = np.sum(-np.log(q[np.arange(N), y])) / N + 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
        "\n",
        "    # Backward pass: calculo de gradientes\n",
        "    grads = {}    \n",
        "    dscores = q\n",
        "    dscores[range(N), y] -= 1\n",
        "    dscores /= N\n",
        "    \n",
        "    # scores = H1W2 + b2\n",
        "    dH1W2 = dscores\n",
        "    db2 = np.sum(dscores, axis=0)\n",
        "    # H1W2 = np.dot(H1, W2)\n",
        "    dH1 = dH1W2.dot(W2.T)\n",
        "    dW2 = H1.T.dot(dH1W2)\n",
        "    # H1 = np.maximum(0, XW1pb1)\n",
        "    dXW1pb1 = dH1\n",
        "    dXW1pb1[XW1pb1 < 0] = 0\n",
        "    # XW1pb1 = XW1 + b1\n",
        "    dXW1 = dXW1pb1\n",
        "    db1 = np.sum(dXW1pb1, axis=0)\n",
        "    # XW1 = np.dot(X, W1)\n",
        "    dW1 = X.T.dot(dXW1)\n",
        "\n",
        "    grads['W1'] = dW1 + reg * W1\n",
        "    grads['b1'] = db1\n",
        "    grads['W2'] = dW2 + reg * W2\n",
        "    grads['b2'] = db2\n",
        "    \n",
        "    return loss, grads\n",
        "\n",
        "  def train(self, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=1e-5, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "\n",
        "    num_train = X.shape[0]\n",
        "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "    # Descenso por gradiente\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for it in range(num_iters):\n",
        "      X_batch = None\n",
        "      y_batch = None\n",
        "\n",
        "      idxs = np.random.choice(num_train, batch_size)\n",
        "      X_batch = X[idxs, :]\n",
        "      y_batch = y[idxs]\n",
        "\n",
        "      # Calculo del Loss y de los gradientes para el minibatch actual\n",
        "      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      self.params['W1'] = self.params['W1'] - learning_rate * grads['W1']\n",
        "      self.params['b1'] = self.params['b1'] - learning_rate * grads['b1']\n",
        "      self.params['W2'] = self.params['W2'] - learning_rate * grads['W2']\n",
        "      self.params['b2'] = self.params['b2'] - learning_rate * grads['b2']\n",
        "\n",
        "      if verbose and it % 100 == 0:\n",
        "        print('iteracion %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "      # Ajuste del learning rate\n",
        "      if it % iterations_per_epoch == 0:\n",
        "        # Checkeo de precision\n",
        "        train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "        val_acc = (self.predict(X_val) == y_val).mean()\n",
        "        train_acc_history.append(train_acc)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        # Reducción del learning rate\n",
        "        learning_rate *= learning_rate_decay\n",
        "\n",
        "    return {\n",
        "      'evolucion_loss': loss_history,\n",
        "      'evolucion_precision_entrenamiento': train_acc_history,\n",
        "      'evolucion_precision_validacion': val_acc_history,\n",
        "    }\n",
        "\n",
        "  def predict(self, X):\n",
        "    y_pred = None\n",
        "    W1, b1 = self.params['W1'], self.params['b1']\n",
        "    W2, b2 = self.params['W2'], self.params['b2']\n",
        "    scores = np.dot(np.maximum(0, np.dot(X, W1) + b1), W2) + b2\n",
        "    y_pred = np.argmax(scores, axis=1)\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH1f22DmydeT",
        "colab_type": "text"
      },
      "source": [
        "## Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjvjx8MQydeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data_utils import load_CIFAR10\n",
        "\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
        "\n",
        "    # Cargados de datos\n",
        "    cifar10_dir = 'cifar-10-batches-py'\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "        \n",
        "    # Sub-muestreo\n",
        "    mask = range(num_training, num_training + num_validation)\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = range(num_training)\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = range(num_test)\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalización\n",
        "    mean_image = np.mean(X_train, axis=0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "\n",
        "    # Cambio de dimensiones\n",
        "    X_train = X_train.reshape(num_training, -1)\n",
        "    X_val = X_val.reshape(num_validation, -1)\n",
        "    X_test = X_test.reshape(num_test, -1)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
        "print('Tamaño de datos de entrenamiento: ', X_train.shape)\n",
        "print('Tamaño de etiquetas de entrenamiento: ', y_train.shape)\n",
        "print('Tamaño de datos de validación: ', X_val.shape)\n",
        "print('Tamaño de etiquetas de validación: ', y_val.shape)\n",
        "print('Tamaño de datos de test: ', X_test.shape)\n",
        "print('Tamaño de etiquetas de test: ', y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVjEGv40ydeY",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "Tiene algunos detalles, como utilizar un descenso de gradiente estocástico con \"momentum\", e ir adaptando el `learning rate` exponencialmente, y ajustado después de cada época.\n",
        "\n",
        "### ¿Por qué será necesario ir ajustando el `learning rate`?\n",
        "\n",
        "Con learning rates bajos, vamos a avanzar muy lento en el descenso por gradiente, logrando mejoras lentas. El loss decaerá más o menos de forma lineal. Por otro lado, con learning rates grandes veremos un decaimiento del loss con forma exponencial. Si es demasiado grande, el loss quedará estancado en un valor más alto, ya que los parámetros quedarán \"saltando\", sin lograr encontrar un punto de estabilidad. Una buena estrategia es entonces empezar con un learning rate grande para dar los saltos más grandes rápidamente, y a medida que buscamos una solución más exacta ir disminuyendo el paso para lograr una mejor estabilidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voZDSnKIydeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 32 * 32 * 3\n",
        "hidden_size = 50\n",
        "num_classes = 10\n",
        "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Entrenamiento\n",
        "stats = net.train(X_train, y_train, X_val, y_val,\n",
        "            num_iters=1500, batch_size=200,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=0.7, verbose=True)\n",
        "\n",
        "# Predicción en el set de validación\n",
        "val_acc = (net.predict(X_val) == y_val).mean()\n",
        "print('Precisión en validación: ', val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODJlBmF_ydec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grafica de la evolución del Loss y de la precisión\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(stats['evolucion_loss'])\n",
        "plt.title('Evolución del Loss')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(stats['evolucion_precision_entrenamiento'], label='train')\n",
        "plt.plot(stats['evolucion_precision_validacion'], label='val')\n",
        "plt.title('Evolución de la precisión de la clasificación')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Precisión')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZEp6Bv9ydef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_acc = (net.predict(X_test) == y_test).mean()\n",
        "print('Precisión en el conjunto de test: ', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJuL7tfydei",
        "colab_type": "text"
      },
      "source": [
        "## ¡FIN!"
      ]
    }
  ]
}